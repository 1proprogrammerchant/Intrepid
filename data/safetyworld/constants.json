{
"learning_rate": 0.001,
"num_homing_policy": 2,
"batch_size": 32,
"max_episodes": 500,
"sabre_finetune": 1,
"model_type": "safe_ff",
"eps_clip": 0.1,
"num_ppo_updates": 10,
"rnd_bonus_coeff": 0,
"entropy_coeff": 0.01,
"encoder_training_num_samples": 20000,
"encoder_training_epoch": 200,
"num_processes": 1,
"forwardmodel": "forwardmodel",
"backwardmodel": "backwardmodel",
"discretization": true,
"policy_type": "safe_ff",
"encoder_training_lr": 0.001,
"encoder_training_batch_size": 32,
"validation_data_percent": 0.2,
"psdp_training_num_samples": 20000,
"cb_oracle_epoch": 40,
"cb_oracle_lr": 0.001,
"cb_oracle_batch_size": 32,
"cb_validation_pct": 0.2,
"cb_patience": 5,
"eval_homing_policy_sample_size": 100,
"n_hidden": 56,
"entropy_reg_coeff": 0.075,
"bootstrap_encoder_model": false,
"failed_homing_policy_filter": false,
"encoder_sampling_style": "reuse",
"data_aggregation": false,
"reward_free_planner": "gps",
"reward_sensitive_planner": "fqi",
"patience": 10,
"bias_homing_policy": false,
"entropy_policy": "none",
"filter_unreachable_abstract_states": false,
"filter_old_abstract_states": false,
"use_l1_penalty": false,
"expected_optima": 0.685,
"max_try": 10,
"reward_type": "stochastic",
"count_type": "state-action",
"clustering_threshold": 0.15
}
